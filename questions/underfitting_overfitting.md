---
layout: default
title: "Underfitting vs Overfitting"
---

# 📌 과소적합(Underfitting)과 과대적합(Overfitting)

---

## 🔷 1. 과소적합 (Underfitting)
모델이 데이터의 패턴을 충분히 학습하지 못하는 상태

- ✅ 학습 데이터와 테스트 데이터 모두에서 성능이 낮음.
- ✅ 너무 단순한 모델(ex: 선형 모델)이 복잡한 패턴을 학습하지 못할 때 발생.

### 🛠 해결법
- ✅ **더 복잡한 모델 사용**  
  - 신경망 층을 늘리거나, 더 강력한 알고리즘 사용 (ex: 선형 모델 → 딥러닝)
- ✅ **더 많은 Feature 추가**  
  - 데이터에서 더 많은 정보를 활용하도록 Feature Engineering 수행
- ✅ **더 많은 학습 데이터 확보**  
  - 학습량이 충분히 이루어지지 않았을 수도 있음
- ✅ **데이터 전처리 개선**  
  - 정규화(Normalization) 및 표준화(Standardization) 적용

---

## 🔶 2. 과대적합 (Overfitting)
모델이 훈련 데이터에 너무 최적화되어 테스트 데이터에서 성능이 떨어지는 현상

- ✅ 훈련 데이터에서는 높은 성능을 내지만, 일반화가 안됨.

### 🛠 해결법
- ✅ **정규화 적용 (Regularization)**  
  - L1 / L2 정규화 (Lasso, Ridge) 또는 Dropout 적용
- ✅ **더 많은 데이터 확보**  
  - 데이터가 적을 경우, 모델이 특정 패턴을 외워버릴 가능성이 높음
- ✅ **모델 복잡도 줄이기**  
  - 신경망 층 수, 뉴런 수 줄이기
- ✅ **Early Stopping 사용**  
  - 검증 데이터의 성능이 더 이상 증가하지 않으면 학습 중단
- ✅ **Batch Normalization 적용**  
  - 신경망 뉴런의 값들이 너무 커지거나 작아지지 않도록 정규화
- ✅ **앙상블 학습 적용**  
  - 여러 모델을 결합하여 과적합을 방지하는 방법 (Random Forest, Bagging, Boosting)

---

## 📌 결론
- 🔹 **과소적합은 모델이 너무 단순하여 충분한 학습을 하지 못하는 문제**  
- 🔹 **과대적합은 모델이 너무 복잡하여 훈련 데이터에 과적합하는 문제**  
- 🔹 **적절한 Regularization, Feature Engineering, 모델 구조 조정이 필요**  

[🏠 홈으로 돌아가기](../index.md)
