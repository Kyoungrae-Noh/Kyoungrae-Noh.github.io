<!DOCTYPE html>
<html lang="ko">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent</title>
    <link rel="stylesheet" href="../styles.css">
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.13/katex.min.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.13/contrib/auto-render.min.js" 
        onload="renderMathInElement(document.body);"></script>
</head>
<body>
    <div class="container">
        <header>
            <h1>📌 경사하강법 (Gradient Descent)</h1>
            <p>최적화하고자 하는 함수 \( J(\theta) \)가 주어졌을 때, 그 함수의 기울기(Gradient, \( \nabla J(\theta) \))를 이용해 변수를 조정하여 최소값을 찾는 과정입니다.</p>
        </header>

        <section class="formula-box">
            <p>\[
            \theta := \theta - \alpha \nabla J(\theta)
            \]</p>
            <ul>
                <li><strong>θ:</strong> 학습할 파라미터 (weight, bias)</li>
                <li><strong>α:</strong> Learning rate</li>
                <li><strong>\( \nabla J(\theta) \):</strong> 손실 함수의 기울기 (Gradient)</li>
            </ul>
        </section>

        <section>
            <h2>📘 경사하강법은 언제 사용되는가?</h2>
            <p>최적화가 필요한 **모든 머신러닝 및 딥러닝 모델**에서 사용됩니다.</p>
            <ul>
                <li><strong>선형회귀 & 로지스틱 회귀:</strong> 가중치를 최적화하여 비용함수를 최소화할 때.</li>
                <li><strong>신경망 학습 (Deep Learning):</strong> 역전파(Backpropagation)와 함께 경사하강법을 사용하여 가중치를 업데이트.</li>
                <li><strong>SVM, PCA 등의 다양한 ML 알고리즘:</strong> 복잡한 최적화 문제에서도 사용됨.</li>
                <li><strong>비용함수 미분이 가능한 경우:</strong> 기울기를 구할 수 있을 때 효과적으로 사용됨.</li>
            </ul>
        </section>

        <section>
            <h2>🔢 경사하강법의 종류</h2>

            <h3>📍 1. 배치 경사하강법 (Batch Gradient Descent)</h3>
            <p>전체 데이터를 한 번에 기울기를 계산하고 업데이트.</p>
            <p>장점: 안정적이지만 데이터가 많을 경우 계산량이 많아 느려질 수 있음.</p>
            <p>\[
            \theta := \theta - \alpha \frac{1}{m} \sum_{i=1}^{m} \nabla J(\theta)
            \]</p>
            <p><i>m: 모든 샘플의 개수</i></p>

            <h3>📍 2. 확률적 경사하강법 (Stochastic Gradient Descent; SGD)</h3>
            <p>하나의 샘플만 사용하여 기울기를 계산하고 업데이트.</p>
            <p>장점: 빠르지만 노이즈가 커서 최적점 근처에서 계속 진동.</p>
            <p>\[
            \theta := \theta - \alpha \nabla J(\theta)
            \]</p>

            <h3>📍 3. 미니배치 경사하강법 (Mini-batch Gradient Descent)</h3>
            <p>여러 개의 샘플(Mini-batch)로 기울기를 계산하고 업데이트.</p>
            <p>배치 방법과 확률적 방법의 장점을 적절히 조합한 방식.</p>
            <p>\[
            \theta := \theta - \alpha \frac{1}{k} \sum_{i=1}^{k} \nabla J(\theta)
            \]</p>
            <p><i>k: 미니배치 크기 (보통 32 ~ 256)</i></p>
        </section>

        <section class="conclusion">
            <h2>📌 결론</h2>
            <ul>
                <li><strong>경사하강법은 머신러닝과 딥러닝에서 최적화를 위한 핵심 알고리즘</strong></li>
                <li><strong>Batch, Stochastic, Mini-batch</strong> 세 가지 방식이 존재하며, 상황에 맞게 선택 가능</li>
                <li><strong>모델 학습의 속도와 안정성을 조절하기 위해 Learning rate 등의 하이퍼파라미터 조정이 중요</strong></li>
            </ul>
        </section>

        <a href="../index.html" class="button">🏠 Back to Home</a>
    </div>
</body>
</html>

